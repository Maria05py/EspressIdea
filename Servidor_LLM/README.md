# üß© Descripci√≥n del C√≥digo

Este script implementa un **servidor web Flask** que expone una **API REST** para generar c√≥digo en **CircuitPython** orientado a la placa **IdeaBoard** mediante un modelo de lenguaje grande (**LLM**).  
Su prop√≥sito principal es **asistir a estudiantes, educadores y desarrolladores** en la creaci√≥n de programas embebidos, proporcionando explicaciones y ejemplos claros, que se almacenan de manera ordenada en un historial persistente.


# üõ†Ô∏è Estructura Principal del C√≥digo

A continuaci√≥n se describen los componentes y bloques m√°s relevantes:


## 1Ô∏è‚É£ Dependencias

- **Flask**: Framework web para exponer el endpoint `/generar`.
- **subprocess**: Permite ejecutar comandos de terminal (en este caso, para Ollama).
- **json**: Serializa y deserializa historial de conversaciones.
- **google.genai**: Cliente oficial para la API Gemini.
- **datetime**: Marca temporal de cada interacci√≥n.
- **os**: Verificaci√≥n de existencia de historial previo.


## 2Ô∏è‚É£ Variables Globales

- `client`: Cliente de Gemini inicializado con tu API Key.
- `prompt_maestro`: Instrucciones base que definen la personalidad del asistente y el formato de respuesta esperado.



## 3Ô∏è‚É£ Endpoint `/generar`

Este endpoint acepta peticiones **POST** con un cuerpo JSON que debe incluir:

```json
{
  "mensaje": "Texto de la consulta o petici√≥n",
  "modelo": "gemini" // o "ollama"
}
```

üìÑ Para probar la invocaci√≥n y llamadas al LLM desde PowerShell, se debe utilizar la siguiente estructura:
```
$body = @{
    mensaje = "PROMPT DESEADO"
    modelo = "gemini"
} | ConvertTo-Json

$response = Invoke-RestMethod `
  -Uri "http://127.0.0.1:5000/generar" `
  -Method POST `
  -Headers @{ "Content-Type" = "application/json" } `
  -Body $body

$response.codigo
$response.explicacion
```

ü¶ô Usando Ollama:
```
$body = @{
    mensaje = "PROMPT DESEADO"
    modelo = "ollama"
} | ConvertTo-Json

$response = Invoke-RestMethod `
  -Uri "http://127.0.0.1:5000/generar" `
  -Method POST `
  -Headers @{ "Content-Type" = "application/json" } `
  -Body $body

$response.codigo
$response.explicacion
```


### üîπ Flujo detallado

**Carga del historial:**

- Si existe `historial.json`, se leen las entradas previas.
- Si no existe, se inicializa vac√≠o.

**Contextualizaci√≥n:**

- Se toman las √∫ltimas tres interacciones (si las hay).
- Se incorporan al prompt como ‚Äúhistorial conversacional‚Äù.

**Ejecuci√≥n del modelo:**

- Si se selecciona `gemini`, se hace una llamada directa con `client.models.generate_content`.
- Si se selecciona `ollama`, se invoca el comando CLI de Ollama.

**Procesamiento de la respuesta:**

- El texto se divide en `<Explicacion>` y `<Codigo>`.
- Ambos se almacenan por separado.

**Registro:**

Cada interacci√≥n se guarda en `historial.json` con:

- Fecha y hora.
- Mensaje original.
- Modelo usado.
- Respuesta estructurada.

**Respuesta HTTP:**

Devuelve un objeto JSON con las secciones:

- `explicacion`
- `codigo`

---

# üìÇ Archivo `historial.json`

Este archivo act√∫a como una **bit√°cora persistente** de todas las consultas realizadas.  
Cada registro incluye:

- `fecha`: marca temporal ISO8601.
- `mensaje`: consulta original.
- `modelo`: ‚Äúgemini‚Äù u ‚Äúollama‚Äù.
- `explicacion`: explicaci√≥n generada por el asistente.
- `codigo`: bloque de c√≥digo generado.

Esto permite:

‚úÖ Mantener contexto en futuras consultas.  
‚úÖ Auditar interacciones pasadas.  
‚úÖ Mejorar trazabilidad del uso de la plataforma.

---

# üîÑ Ejemplo de Respuesta JSON

Una respuesta t√≠pica del endpoint `/generar` tiene el siguiente formato:

```json
{
  "explicacion": "Este ejemplo muestra c√≥mo controlar un servo usando la librer√≠a ideaboard...",
  "codigo": "from ideaboard import IdeaBoard\nib = IdeaBoard()\nservo = ib.Servo(5)\nservo.angle = 90"
}

```
